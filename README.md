# sign-language-project

# About the software

This software is meant for communication with the deaf and dumb people. It reads and classifies 
sign language using a CNN.
Humans know each other by conveying their ideas, thoughts, and experiences to the people around 
them. There are numerous ways to achieve this and the best one among the rest is the gift of 
“Speech”. Through speech everyone can very convincingly transfer their thoughts and understand 
each other. It will be injustice if we ignore those who are deprived of this invaluable gift; the deaf 
and dumb people. The only means of communication available to the deaf and dumb people is the 
use of “Sign Language”. Using sign language, they are limited to their own world. This limitation 
prevents them from interacting with the outer world to share their feelings, creative ideas and 
Potentials. Very few people who are not themselves deaf and dumb ever learn to Sign language.
These limitation increases the isolation of deaf and dumb people from the common society. 
Technology is one way to remove this hindrance and benefit these people.
The communication between a dumb and hearing person poses to be an important disadvantage 
compared to communication between blind and ancient visual people. This creates an extremely 
little house for them with communication being associate degree elementary aspect of human life. 
The blind people can speak freely by implies that of ancient language whereas the dumb have their 
own manual-visual language referred to as sign language. Sign language is also a non-verbal form of 
intercourse that's found among deaf communities at intervals the planet. The sign languages haven't 
got a typical origin and hence hard to interpret. A Dumb communication interpreter is also a tool 
that interprets the hand gestures to sensibility speech. A gesture in associate degree extremely 
language is also a certain movement of the hands with a particular kind created out of them. A 
gesture in a sign language is a particular movement of the hands with a specific shape made out of 
them. A sign language usually provides sign for whole words. It can also provide sign for letters to 
perform words that don’t have corresponding sign in that sign language. In this device Flex Sensor 
plays the major role, Flex sensors are sensors that change in resistance depending on the amount of 
bend on the sensor. This digital glove aims to lower this barrier in communication. It is electronic 
device that can translate Sign language into speech in order to make the communication take place 
between the mute communities with the general public possible. A hand gesture recognition system 
is also used to recognize real time gesture in unconstrained environments. The system consists of 
three modules: real time hand tracking, training gesture and gesture recognition using pseudo two-dimension hidden Markov models. In this they have used a Kalman filter and hand blobs analysis for 
hand tracking to obtain motion descriptors and hand region.
Sign language recognition (SLR) is an evolving research area in computer vision. The challenges in SLR 
are video trimming, sign extraction, sign video background modelling, sign feature representation and 
sign classification. All the problems are attempted in the past have met considerable amount of success 
and are instrumental in development of the state of the algorithms for SLR. Gesture recognition uses 
powerful imaging and artificial intelligence-based algorithms for classification. Current trends show an 
urge to bring gesture recognition into mobile environments. Sign language is visual mode of 
communication between two hearing impaired or hard hearing people. The communication foundations 
are based on finger shapes, hand shapes, hand movements in space with respect to body, hand 
orientations and facial expressions. The humans are trained exclusively to hand such huge amounts of 
information for years. For machine translation, the problem transforms into a 2D natural language 
processing problem. Many 1D/2D/3D models are proposed in literature with little success to bring the 
model close to real time implementation.
Sign language recognition (SLR) has transformed with technology upgradation from 1D, 2D to 3D 
models in the last 2 decades. In 1D, SLR is based on 1D signals acquired from a hand gloves and 
classified using signal processing methods. However, 1D methods are not up to the mark in terms of 
accuracy, and require hardware, complex circuitry, processor, etc. Also, the hand images are 
inherently 2 dimensional or 3 dimensional in nature. If we reduce it to one dimensional electrical 
signal, using some flex sensor, there will be a huge loss in data. With the development of modern 
methods like ML, DL, AI, ANN, CNN, etc. we can deal with higher dimensional image data directly 
through software like Python.

 
# System Requirements

Operating System: Windows

Python Software Distribution: Anaconda 3, Version 2020.11

Software Packages: NumPy*, Pandas, Matplotlib, Seaborn, TensorFlow, Keras, OpenCV, 
PIL, SMTPLIB*, Collections*

Editors: Spyder 4.1.5, Jupyter Notebook

Google Account with SMTP access enabled.

Additional Hardware: Webcam**

*These packages come in-built. They don’t need to be installed. Those without this mark need to be 
installed.

**This is not required for laptops as they have an in-built webcam. For desktop computers, webcam 
is required.

For more details, refer the accompanying ReadMe.pdf file.